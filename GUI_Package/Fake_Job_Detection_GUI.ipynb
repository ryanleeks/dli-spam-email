{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "eMS3stikZ8A0",
        "outputId": "b738197b-d8d2-4f65-f748-204b4acb2ad5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a8c5bd7993c939a57b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://a8c5bd7993c939a57b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# --- 1) Mount Google Drive ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- 2) Point to model folder in Drive ---\n",
        "MODEL_DIR = \"/content/drive/MyDrive/model\"\n",
        "\n",
        "# --- 3) Load tokenizer + model from Drive ---\n",
        "!pip install -q transformers torch gradio\n",
        "\n",
        "import os, torch, torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import gradio as gr\n",
        "\n",
        "assert os.path.isdir(MODEL_DIR), f\"Model dir not found: {MODEL_DIR}. Check the path in Drive.\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)\n",
        "model.eval()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "labels = [\"Legit\", \"Fraud\"]  \n",
        "\n",
        "@torch.no_grad()\n",
        "def predict(text):\n",
        "    enc = tokenizer(text, truncation=True, padding=True, max_length=256, return_tensors=\"pt\").to(device)\n",
        "    outputs = model(**enc)\n",
        "    probs = F.softmax(outputs.logits, dim=-1).squeeze().detach().cpu().tolist()\n",
        "    pred_idx = int(torch.tensor(probs).argmax())\n",
        "    pred = labels[pred_idx]\n",
        "    return pred, {labels[i]: float(probs[i]) for i in range(len(labels))}\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=\"text\",\n",
        "    outputs=[\"label\", \"json\"],\n",
        "    title=\"Fake Job Detector\",\n",
        "    description=\"Paste a job posting to check if it's Legit or Fraud\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
